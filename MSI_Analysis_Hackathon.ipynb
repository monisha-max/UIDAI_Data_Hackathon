{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ‡®ğŸ‡³ Mobility Signal Index (MSI) Analysis\n",
        "## Detecting Redistribution Patterns in Aadhaar Data\n",
        "\n",
        "---\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "**Objective:** Detect \"redistribution-like patterns\" in Aadhaar enrollment and update data without overclaiming actual population movement.\n",
        "\n",
        "**Key Insight:** When one geographic area shows *declining* Aadhaar activity while neighboring areas show *increasing* activity in the same time window, this creates a statistical signal that may indicate population redistribution.\n",
        "\n",
        "### Methodology Overview\n",
        "\n",
        "1. **Mobility Signal Index (MSI):** Measures inverse correlation between an area and its geographic neighbors\n",
        "2. **Wave Propagation Detection:** Identifies patterns that spread spatially over time (start â†’ peak â†’ fade)\n",
        "3. **Local Effect Filtering:** Distinguishes true redistribution from operational effects (e.g., single-pincode camps)\n",
        "\n",
        "### Careful Wording\n",
        "\n",
        "- âœ… \"Redistribution-like patterns detected\"\n",
        "- âœ… \"Possible population movement signals\"\n",
        "- âŒ \"People definitely moved from A to B\"\n",
        "\n",
        "---\n",
        "\n",
        "**Theme:** ğŸ‡®ğŸ‡³ Indian Tricolor (Saffron â€¢ White â€¢ Green)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“¦ IMPORTS & CONFIGURATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pio.renderers.default = 'notebook'\n",
        "\n",
        "print(\"âœ… Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ¨ INDIAN TRICOLOR THEME CONFIGURATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "INDIA_COLORS = {\n",
        "    # Primary tricolor\n",
        "    'saffron': '#FF9933',           # à¤•à¥‡à¤¸à¤°à¥€ - Courage & Sacrifice\n",
        "    'saffron_light': '#FFB366',\n",
        "    'saffron_dark': '#E67300',\n",
        "    'white': '#FFFFFF',              # à¤¶à¤¾à¤‚à¤¤à¤¿ - Peace & Truth\n",
        "    'white_off': '#F8F9FA',\n",
        "    'green': '#138808',              # à¤¹à¤°à¤¾ - Fertility & Prosperity  \n",
        "    'green_light': '#1DB954',\n",
        "    'green_dark': '#0D5C06',\n",
        "    \n",
        "    # Ashoka Chakra\n",
        "    'navy': '#000080',\n",
        "    'chakra_blue': '#0000CD',\n",
        "    \n",
        "    # Background & UI\n",
        "    'background': '#0A0A1A',\n",
        "    'background_light': '#1A1A2E',\n",
        "    'text': '#F0F0F0',\n",
        "    'text_muted': '#A0A0A0',\n",
        "    'grid': '#2A2A4A',\n",
        "    'accent': '#FFD700'  # Gold accent\n",
        "}\n",
        "\n",
        "# Indian tricolor gradient for continuous data\n",
        "INDIA_COLORSCALE = [\n",
        "    [0.0, INDIA_COLORS['green_dark']],\n",
        "    [0.25, INDIA_COLORS['green']],\n",
        "    [0.5, INDIA_COLORS['white']],\n",
        "    [0.75, INDIA_COLORS['saffron']],\n",
        "    [1.0, INDIA_COLORS['saffron_dark']]\n",
        "]\n",
        "\n",
        "# Custom Plotly template\n",
        "INDIA_TEMPLATE = go.layout.Template(\n",
        "    layout=go.Layout(\n",
        "        paper_bgcolor=INDIA_COLORS['background'],\n",
        "        plot_bgcolor=INDIA_COLORS['background'],\n",
        "        font=dict(\n",
        "            family=\"'Segoe UI', 'Helvetica Neue', Arial, sans-serif\",\n",
        "            color=INDIA_COLORS['text'],\n",
        "            size=12\n",
        "        ),\n",
        "        title=dict(\n",
        "            font=dict(size=24, color=INDIA_COLORS['saffron']),\n",
        "            x=0.5,\n",
        "            xanchor='center'\n",
        "        ),\n",
        "        xaxis=dict(\n",
        "            gridcolor=INDIA_COLORS['grid'],\n",
        "            linecolor=INDIA_COLORS['grid'],\n",
        "            zerolinecolor=INDIA_COLORS['grid'],\n",
        "            tickfont=dict(color=INDIA_COLORS['text_muted'])\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            gridcolor=INDIA_COLORS['grid'],\n",
        "            linecolor=INDIA_COLORS['grid'],\n",
        "            zerolinecolor=INDIA_COLORS['grid'],\n",
        "            tickfont=dict(color=INDIA_COLORS['text_muted'])\n",
        "        ),\n",
        "        colorway=[\n",
        "            INDIA_COLORS['saffron'],\n",
        "            INDIA_COLORS['green'],\n",
        "            INDIA_COLORS['chakra_blue'],\n",
        "            INDIA_COLORS['saffron_light'],\n",
        "            INDIA_COLORS['green_light'],\n",
        "            INDIA_COLORS['accent']\n",
        "        ],\n",
        "        hoverlabel=dict(\n",
        "            bgcolor=INDIA_COLORS['background_light'],\n",
        "            font_size=12,\n",
        "            font_family=\"'Segoe UI', Arial\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"ğŸ¨ Indian Tricolor Theme configured!\")\n",
        "print(f\"   ğŸŸ  Saffron: {INDIA_COLORS['saffron']}\")\n",
        "print(f\"   âšª White:   {INDIA_COLORS['white']}\")\n",
        "print(f\"   ğŸŸ¢ Green:   {INDIA_COLORS['green']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1ï¸âƒ£ Data Loading & Preprocessing\n",
        "\n",
        "Loading three Aadhaar datasets:\n",
        "- **Enrolment:** New Aadhaar registrations by age group\n",
        "- **Demographic Updates:** Changes to name, address, DOB, etc.\n",
        "- **Biometric Updates:** Fingerprint, iris, face updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“‚ DATA LOADING\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "BASE_PATH = Path(\".\")\n",
        "\n",
        "print(\"ğŸ”„ Loading Aadhaar datasets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Enrolment Data\n",
        "enrolment_files = sorted(BASE_PATH.glob(\"api_data_aadhar_enrolment/*.csv\"))\n",
        "enrolment_df = pd.concat([pd.read_csv(f) for f in enrolment_files], ignore_index=True)\n",
        "print(f\"ğŸ“‹ Enrolment:   {len(enrolment_df):>12,} records\")\n",
        "\n",
        "# Load Demographic Data  \n",
        "demographic_files = sorted(BASE_PATH.glob(\"api_data_aadhar_demographic/*.csv\"))\n",
        "demographic_df = pd.concat([pd.read_csv(f) for f in demographic_files], ignore_index=True)\n",
        "print(f\"ğŸ“‹ Demographic: {len(demographic_df):>12,} records\")\n",
        "\n",
        "# Load Biometric Data\n",
        "biometric_files = sorted(BASE_PATH.glob(\"api_data_aadhar_biometric/*.csv\"))\n",
        "biometric_df = pd.concat([pd.read_csv(f) for f in biometric_files], ignore_index=True)\n",
        "print(f\"ğŸ“‹ Biometric:   {len(biometric_df):>12,} records\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "total = len(enrolment_df) + len(demographic_df) + len(biometric_df)\n",
        "print(f\"ğŸ“Š TOTAL:       {total:>12,} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ”§ DATA PREPROCESSING\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def preprocess_dataframe(df, dataset_name):\n",
        "    \"\"\"Clean and enhance a single dataframe\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Parse dates\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
        "    \n",
        "    # Clean geographic fields\n",
        "    df['state'] = df['state'].str.strip().str.title()\n",
        "    df['district'] = df['district'].str.strip().str.title()\n",
        "    \n",
        "    # Pincode processing - extract geographic regions\n",
        "    df['pincode'] = df['pincode'].astype(str).str.zfill(6)\n",
        "    df['pin_region'] = df['pincode'].str[:3]      # First 3 digits = postal region\n",
        "    df['pin_subregion'] = df['pincode'].str[:4]   # First 4 digits = sub-region\n",
        "    \n",
        "    # Time-based columns\n",
        "    df['week'] = df['date'].dt.isocalendar().week\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year_week'] = df['date'].dt.strftime('%Y-W%V')\n",
        "    df['year_month'] = df['date'].dt.strftime('%Y-%m')\n",
        "    \n",
        "    # Create geo_key for easy aggregation\n",
        "    df['geo_key'] = df['state'] + '|' + df['district']\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"ğŸ”§ Preprocessing datasets...\")\n",
        "\n",
        "enrolment_df = preprocess_dataframe(enrolment_df, 'enrolment')\n",
        "demographic_df = preprocess_dataframe(demographic_df, 'demographic')\n",
        "biometric_df = preprocess_dataframe(biometric_df, 'biometric')\n",
        "\n",
        "# Create total activity columns\n",
        "enrolment_df['total_enrolment'] = (\n",
        "    enrolment_df['age_0_5'] + \n",
        "    enrolment_df['age_5_17'] + \n",
        "    enrolment_df['age_18_greater']\n",
        ")\n",
        "\n",
        "demographic_df['total_demo_updates'] = (\n",
        "    demographic_df['demo_age_5_17'] + \n",
        "    demographic_df['demo_age_17_']\n",
        ")\n",
        "\n",
        "biometric_df['total_bio_updates'] = (\n",
        "    biometric_df['bio_age_5_17'] + \n",
        "    biometric_df['bio_age_17_']\n",
        ")\n",
        "\n",
        "print(\"âœ… Preprocessing complete!\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nğŸ“‹ Sample Enrolment Data:\")\n",
        "enrolment_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š DATA OVERVIEW\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Date range info\n",
        "print(\"ğŸ“… Date Ranges:\")\n",
        "print(f\"   Enrolment:   {enrolment_df['date'].min().strftime('%d-%b-%Y')} â†’ {enrolment_df['date'].max().strftime('%d-%b-%Y')}\")\n",
        "print(f\"   Demographic: {demographic_df['date'].min().strftime('%d-%b-%Y')} â†’ {demographic_df['date'].max().strftime('%d-%b-%Y')}\")\n",
        "print(f\"   Biometric:   {biometric_df['date'].min().strftime('%d-%b-%Y')} â†’ {biometric_df['date'].max().strftime('%d-%b-%Y')}\")\n",
        "\n",
        "# Geographic coverage\n",
        "states = set(enrolment_df['state'].unique()) | set(demographic_df['state'].unique()) | set(biometric_df['state'].unique())\n",
        "districts = set(enrolment_df['district'].unique()) | set(demographic_df['district'].unique()) | set(biometric_df['district'].unique())\n",
        "pincodes = set(enrolment_df['pincode'].unique()) | set(demographic_df['pincode'].unique()) | set(biometric_df['pincode'].unique())\n",
        "\n",
        "print(f\"\\nğŸ—ºï¸  Geographic Coverage:\")\n",
        "print(f\"   States:    {len(states)}\")\n",
        "print(f\"   Districts: {len(districts)}\")\n",
        "print(f\"   Pincodes:  {len(pincodes):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ”— CREATE COMBINED ACTIVITY DATASET\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating combined activity dataset...\")\n",
        "\n",
        "# Aggregate each dataset by geographic and temporal keys\n",
        "agg_cols = ['date', 'state', 'district', 'geo_key', 'pin_region', 'year_week']\n",
        "\n",
        "enrol_agg = enrolment_df.groupby(agg_cols)['total_enrolment'].sum().reset_index()\n",
        "enrol_agg.rename(columns={'total_enrolment': 'enrolment'}, inplace=True)\n",
        "\n",
        "demo_agg = demographic_df.groupby(agg_cols)['total_demo_updates'].sum().reset_index()\n",
        "demo_agg.rename(columns={'total_demo_updates': 'demo_updates'}, inplace=True)\n",
        "\n",
        "bio_agg = biometric_df.groupby(agg_cols)['total_bio_updates'].sum().reset_index()\n",
        "bio_agg.rename(columns={'total_bio_updates': 'bio_updates'}, inplace=True)\n",
        "\n",
        "# Merge all datasets\n",
        "combined_df = enrol_agg.merge(\n",
        "    demo_agg, on=agg_cols, how='outer'\n",
        ").merge(\n",
        "    bio_agg, on=agg_cols, how='outer'\n",
        ").fillna(0)\n",
        "\n",
        "# Total activity = sum of all types\n",
        "combined_df['total_activity'] = (\n",
        "    combined_df['enrolment'] + \n",
        "    combined_df['demo_updates'] + \n",
        "    combined_df['bio_updates']\n",
        ")\n",
        "\n",
        "print(f\"âœ… Combined dataset: {len(combined_df):,} records\")\n",
        "print(f\"   Unique geo_keys: {combined_df['geo_key'].nunique()}\")\n",
        "print(f\"   Time periods: {combined_df['year_week'].nunique()}\")\n",
        "\n",
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2ï¸âƒ£ Mobility Signal Index (MSI) Computation\n",
        "\n",
        "### MSI Formula\n",
        "\n",
        "$$MSI(A, t) = -\\rho(\\Delta_{activity,A}, \\overline{\\Delta_{activity,neighbors}}) \\times (1 + S_{spread}) \\times \\frac{\\min(|Z|, 3)}{3}$$\n",
        "\n",
        "Where:\n",
        "- $\\rho$ = Pearson correlation between location A's change and average neighbor change\n",
        "- $S_{spread}$ = Proportion of neighbors changing in opposite direction\n",
        "- $Z$ = Z-score of the activity change (anomaly magnitude)\n",
        "\n",
        "**Interpretation:**\n",
        "- **High MSI (> 0.3):** Strong redistribution signal\n",
        "- **MSI â‰ˆ 0:** No clear pattern\n",
        "- **Negative MSI:** Correlated movement (everyone moving together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ—ºï¸  BUILD GEOGRAPHIC NEIGHBOR GRAPH\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ—ºï¸  Building geographic neighbor relationships...\")\n",
        "\n",
        "# Districts within the same state are considered neighbors\n",
        "state_districts = combined_df.groupby('state')['district'].unique().to_dict()\n",
        "\n",
        "district_neighbors = {}\n",
        "for state, dists in state_districts.items():\n",
        "    dists = list(dists)\n",
        "    for d1 in dists:\n",
        "        key = f\"{state}|{d1}\"\n",
        "        district_neighbors[key] = [f\"{state}|{d2}\" for d2 in dists if d2 != d1]\n",
        "\n",
        "print(f\"âœ… Built neighbor graph with {len(district_neighbors)} nodes\")\n",
        "print(f\"   Average neighbors per district: {np.mean([len(v) for v in district_neighbors.values()]):.1f}\")\n",
        "\n",
        "# Show example\n",
        "example_key = list(district_neighbors.keys())[0]\n",
        "print(f\"\\n   Example: {example_key}\")\n",
        "print(f\"   Neighbors: {district_neighbors[example_key][:5]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“ˆ COMPUTE TEMPORAL CHANGES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“ˆ Computing temporal activity changes...\")\n",
        "\n",
        "# Aggregate by week and geographic unit\n",
        "weekly_activity = combined_df.groupby(['year_week', 'geo_key'])['total_activity'].sum().reset_index()\n",
        "\n",
        "# Pivot to get time series per location\n",
        "activity_pivot = weekly_activity.pivot(\n",
        "    index='year_week', \n",
        "    columns='geo_key', \n",
        "    values='total_activity'\n",
        ").fillna(0)\n",
        "\n",
        "# Sort by date\n",
        "activity_pivot = activity_pivot.sort_index()\n",
        "\n",
        "print(f\"âœ… Activity matrix: {activity_pivot.shape[0]} weeks Ã— {activity_pivot.shape[1]} locations\")\n",
        "\n",
        "# Compute percentage changes (week-over-week)\n",
        "pct_changes = activity_pivot.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "# Compute rolling z-scores for anomaly detection\n",
        "rolling_mean = activity_pivot.rolling(window=4, min_periods=1).mean()\n",
        "rolling_std = activity_pivot.rolling(window=4, min_periods=1).std().replace(0, 1)\n",
        "z_scores = (activity_pivot - rolling_mean) / rolling_std\n",
        "\n",
        "print(f\"âœ… Computed percentage changes and z-scores\")\n",
        "\n",
        "# Preview\n",
        "print(f\"\\nğŸ“Š Sample activity trends (first 5 locations, last 5 weeks):\")\n",
        "activity_pivot.iloc[-5:, :5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ§® COMPUTE MOBILITY SIGNAL INDEX (MSI)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ§® Computing Mobility Signal Index...\")\n",
        "print(\"   This may take a moment for large datasets...\")\n",
        "\n",
        "WINDOW_SIZE = 3  # Number of weeks for rolling correlation\n",
        "\n",
        "msi_records = []\n",
        "locations = list(pct_changes.columns)\n",
        "time_periods = list(pct_changes.index)\n",
        "\n",
        "processed = 0\n",
        "for loc in locations:\n",
        "    # Get neighbors\n",
        "    neighbors = district_neighbors.get(loc, [])\n",
        "    valid_neighbors = [n for n in neighbors if n in locations]\n",
        "    \n",
        "    if len(valid_neighbors) < 2:\n",
        "        continue\n",
        "    \n",
        "    for t_idx in range(WINDOW_SIZE, len(time_periods)):\n",
        "        t = time_periods[t_idx]\n",
        "        t_window = time_periods[t_idx - WINDOW_SIZE:t_idx + 1]\n",
        "        \n",
        "        # Get changes for this location in the window\n",
        "        loc_changes = pct_changes.loc[t_window, loc].values\n",
        "        \n",
        "        # Get average neighbor changes in the window\n",
        "        neighbor_changes = pct_changes.loc[t_window, valid_neighbors].mean(axis=1).values\n",
        "        \n",
        "        # MSI Component 1: Inverse correlation\n",
        "        if np.std(loc_changes) > 0.001 and np.std(neighbor_changes) > 0.001:\n",
        "            correlation = np.corrcoef(loc_changes, neighbor_changes)[0, 1]\n",
        "            if np.isnan(correlation):\n",
        "                correlation = 0\n",
        "            inverse_corr = -correlation  # Negative correlation = redistribution\n",
        "        else:\n",
        "            inverse_corr = 0\n",
        "        \n",
        "        # MSI Component 2: Spatial spread factor\n",
        "        loc_direction = np.sign(loc_changes[-1]) if abs(loc_changes[-1]) > 0.01 else 0\n",
        "        if loc_direction != 0:\n",
        "            neighbor_directions = np.sign(pct_changes.loc[t, valid_neighbors].values)\n",
        "            opposite_count = np.sum(neighbor_directions != loc_direction)\n",
        "            spatial_spread = opposite_count / len(valid_neighbors)\n",
        "        else:\n",
        "            spatial_spread = 0\n",
        "            opposite_count = 0\n",
        "        \n",
        "        # MSI Component 3: Anomaly magnitude (z-score)\n",
        "        z_magnitude = abs(z_scores.loc[t, loc]) if t in z_scores.index else 0\n",
        "        z_magnitude = min(z_magnitude, 3)  # Cap at 3\n",
        "        \n",
        "        # Final MSI calculation\n",
        "        msi = inverse_corr * (1 + spatial_spread) * (z_magnitude / 3)\n",
        "        \n",
        "        # Store record\n",
        "        state, district = loc.split('|')\n",
        "        msi_records.append({\n",
        "            'time_period': t,\n",
        "            'state': state,\n",
        "            'district': district,\n",
        "            'geo_key': loc,\n",
        "            'msi_score': msi,\n",
        "            'inverse_correlation': inverse_corr,\n",
        "            'spatial_spread': spatial_spread,\n",
        "            'z_magnitude': z_magnitude,\n",
        "            'activity_change_pct': loc_changes[-1] * 100,\n",
        "            'neighbor_change_pct': neighbor_changes[-1] * 100,\n",
        "            'activity_level': activity_pivot.loc[t, loc],\n",
        "            'num_neighbors': len(valid_neighbors),\n",
        "            'neighbors_opposite': opposite_count\n",
        "        })\n",
        "    \n",
        "    processed += 1\n",
        "    if processed % 200 == 0:\n",
        "        print(f\"   Processed {processed}/{len(locations)} locations...\")\n",
        "\n",
        "# Create MSI DataFrame\n",
        "msi_df = pd.DataFrame(msi_records)\n",
        "\n",
        "print(f\"\\nâœ… MSI computation complete!\")\n",
        "print(f\"   Total measurements: {len(msi_df):,}\")\n",
        "print(f\"   High-MSI events (>0.3): {len(msi_df[msi_df['msi_score'] > 0.3]):,}\")\n",
        "\n",
        "msi_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Visualizations ğŸ‡®ğŸ‡³\n",
        "\n",
        "All visualizations use the **Indian Tricolor theme**:\n",
        "- ğŸŸ  **Saffron:** High MSI / High activity / Redistribution signals\n",
        "- âšª **White:** Neutral / Baseline\n",
        "- ğŸŸ¢ **Green:** Low MSI / Normal activity / Stable patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 1: EXECUTIVE SUMMARY DASHBOARD\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating Executive Summary Dashboard...\")\n",
        "\n",
        "# Key metrics - Focus on POSITIVE MSI (redistribution signals)\n",
        "high_msi_events = msi_df[msi_df['msi_score'] > 0.3]\n",
        "high_msi_count = len(high_msi_events)\n",
        "unique_locations = msi_df['geo_key'].nunique()\n",
        "max_msi = msi_df['msi_score'].max()\n",
        "\n",
        "fig_summary = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=(\n",
        "        \"ğŸ¯ Redistribution Events (MSI > 0.3)\",\n",
        "        \"ğŸ—ºï¸ Locations Analyzed\",\n",
        "        \"ğŸ“ˆ Peak MSI Score\",\n",
        "        \"ğŸ† States with Most Redistribution Events\",\n",
        "        \"ğŸ“Š MSI Score Distribution\",\n",
        "        \"ğŸŒ Spatial Spread Distribution\"\n",
        "    ),\n",
        "    specs=[\n",
        "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
        "        [{\"type\": \"bar\"}, {\"type\": \"histogram\"}, {\"type\": \"histogram\"}]\n",
        "    ],\n",
        "    vertical_spacing=0.18,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "# Indicators\n",
        "fig_summary.add_trace(\n",
        "    go.Indicator(\n",
        "        mode=\"number\",\n",
        "        value=high_msi_count,\n",
        "        number={'font': {'color': INDIA_COLORS['saffron'], 'size': 48}}\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "fig_summary.add_trace(\n",
        "    go.Indicator(\n",
        "        mode=\"number\",\n",
        "        value=unique_locations,\n",
        "        number={'font': {'color': INDIA_COLORS['green'], 'size': 48}}\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig_summary.add_trace(\n",
        "    go.Indicator(\n",
        "        mode=\"number\",\n",
        "        value=max_msi,\n",
        "        number={'font': {'color': INDIA_COLORS['saffron'], 'size': 48}, 'valueformat': '.4f'}\n",
        "    ),\n",
        "    row=1, col=3\n",
        ")\n",
        "\n",
        "# Top states BY REDISTRIBUTION EVENT COUNT (not mean MSI)\n",
        "state_event_counts = high_msi_events.groupby('state').size().sort_values(ascending=False).head(10)\n",
        "fig_summary.add_trace(\n",
        "    go.Bar(\n",
        "        x=state_event_counts.index,\n",
        "        y=state_event_counts.values,\n",
        "        marker=dict(\n",
        "            color=state_event_counts.values,\n",
        "            colorscale=INDIA_COLORSCALE\n",
        "        ),\n",
        "        text=state_event_counts.values,\n",
        "        textposition='outside',\n",
        "        textfont=dict(color=INDIA_COLORS['text'], size=10)\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# MSI distribution\n",
        "fig_summary.add_trace(\n",
        "    go.Histogram(\n",
        "        x=msi_df['msi_score'],\n",
        "        nbinsx=50,\n",
        "        marker=dict(\n",
        "            color=INDIA_COLORS['saffron'],\n",
        "            line=dict(color=INDIA_COLORS['saffron_dark'], width=1)\n",
        "        )\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Spatial spread distribution\n",
        "fig_summary.add_trace(\n",
        "    go.Histogram(\n",
        "        x=msi_df['spatial_spread'],\n",
        "        nbinsx=30,\n",
        "        marker=dict(\n",
        "            color=INDIA_COLORS['green'],\n",
        "            line=dict(color=INDIA_COLORS['green_dark'], width=1)\n",
        "        )\n",
        "    ),\n",
        "    row=2, col=3\n",
        ")\n",
        "\n",
        "fig_summary.update_layout(\n",
        "    template=INDIA_TEMPLATE,\n",
        "    title=dict(\n",
        "        text=\"ğŸ‡®ğŸ‡³ MOBILITY SIGNAL INDEX â€” EXECUTIVE SUMMARY\",\n",
        "        font=dict(size=28, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    height=650,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig_summary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 2: MSI HEATMAP BY STATE & TIME\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating MSI Temporal Heatmap...\")\n",
        "\n",
        "# Aggregate by state and time\n",
        "heatmap_data = msi_df.groupby(['time_period', 'state'])['msi_score'].mean().reset_index()\n",
        "heatmap_pivot = heatmap_data.pivot(\n",
        "    index='state',\n",
        "    columns='time_period',\n",
        "    values='msi_score'\n",
        ").fillna(0)\n",
        "\n",
        "# Sort states by total MSI\n",
        "state_order = heatmap_pivot.sum(axis=1).sort_values(ascending=True).index\n",
        "heatmap_pivot = heatmap_pivot.loc[state_order]\n",
        "\n",
        "fig_heatmap = go.Figure(data=go.Heatmap(\n",
        "    z=heatmap_pivot.values,\n",
        "    x=heatmap_pivot.columns,\n",
        "    y=heatmap_pivot.index,\n",
        "    colorscale=INDIA_COLORSCALE,\n",
        "    colorbar=dict(\n",
        "        title=dict(text=\"MSI Score\", font=dict(color=INDIA_COLORS['text'])),\n",
        "        tickfont=dict(color=INDIA_COLORS['text']),\n",
        "        len=0.9\n",
        "    ),\n",
        "    hovertemplate=\"State: %{y}<br>Week: %{x}<br>MSI: %{z:.4f}<extra></extra>\"\n",
        "))\n",
        "\n",
        "fig_heatmap.update_layout(\n",
        "    template=INDIA_TEMPLATE,\n",
        "    title=dict(\n",
        "        text=\"ğŸ—“ï¸ MSI Temporal Heatmap: When Did Redistribution Patterns Peak?\",\n",
        "        font=dict(size=22, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    xaxis_title=\"Week\",\n",
        "    yaxis_title=\"State\",\n",
        "    height=900,\n",
        "    xaxis=dict(tickangle=45)\n",
        ")\n",
        "\n",
        "fig_heatmap.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 3: TOP REDISTRIBUTION HOTSPOTS (RANKED)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating Hotspot Ranking...\")\n",
        "\n",
        "# Aggregate MSI by location\n",
        "hotspot_stats = msi_df.groupby(['state', 'district', 'geo_key']).agg({\n",
        "    'msi_score': ['mean', 'max', 'std', 'count'],\n",
        "    'inverse_correlation': 'mean',\n",
        "    'spatial_spread': 'mean',\n",
        "    'z_magnitude': 'mean',\n",
        "    'activity_change_pct': 'mean',\n",
        "    'activity_level': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "hotspot_stats.columns = [\n",
        "    'state', 'district', 'geo_key',\n",
        "    'msi_mean', 'msi_max', 'msi_std', 'event_count',\n",
        "    'avg_inverse_corr', 'avg_spatial_spread', 'avg_z_magnitude',\n",
        "    'avg_activity_change', 'avg_activity_level'\n",
        "]\n",
        "\n",
        "# Composite hotspot score\n",
        "hotspot_stats['hotspot_score'] = (\n",
        "    hotspot_stats['msi_mean'] * 0.3 +\n",
        "    hotspot_stats['msi_max'] * 0.3 +\n",
        "    hotspot_stats['avg_spatial_spread'] * 0.2 +\n",
        "    np.log1p(hotspot_stats['event_count']) * 0.1 +\n",
        "    hotspot_stats['avg_z_magnitude'] / 3 * 0.1\n",
        ")\n",
        "\n",
        "hotspot_stats = hotspot_stats.sort_values('hotspot_score', ascending=False)\n",
        "top_hotspots = hotspot_stats.head(20)\n",
        "\n",
        "# Create horizontal bar chart\n",
        "fig_hotspots = go.Figure()\n",
        "\n",
        "fig_hotspots.add_trace(go.Bar(\n",
        "    y=[f\"{row['district']}, {row['state']}\" for _, row in top_hotspots.iterrows()],\n",
        "    x=top_hotspots['hotspot_score'],\n",
        "    orientation='h',\n",
        "    marker=dict(\n",
        "        color=top_hotspots['hotspot_score'],\n",
        "        colorscale=INDIA_COLORSCALE,\n",
        "        line=dict(color=INDIA_COLORS['saffron_dark'], width=1)\n",
        "    ),\n",
        "    text=[f\"Score: {s:.3f}\" for s in top_hotspots['hotspot_score']],\n",
        "    textposition='inside',\n",
        "    textfont=dict(color=INDIA_COLORS['background'], size=11, family='Arial Black'),\n",
        "    hovertemplate=(\n",
        "        \"<b>%{y}</b><br>\" +\n",
        "        \"Hotspot Score: %{x:.4f}<br>\" +\n",
        "        \"<extra></extra>\"\n",
        "    )\n",
        "))\n",
        "\n",
        "fig_hotspots.update_layout(\n",
        "    template=INDIA_TEMPLATE,\n",
        "    title=dict(\n",
        "        text=\"ğŸ¯ TOP 20 REDISTRIBUTION HOTSPOTS (Ranked by MSI)\",\n",
        "        font=dict(size=22, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    xaxis_title=\"Hotspot Score\",\n",
        "    yaxis_title=\"\",\n",
        "    height=700,\n",
        "    showlegend=False,\n",
        "    yaxis=dict(autorange=\"reversed\")\n",
        ")\n",
        "\n",
        "fig_hotspots.show()\n",
        "\n",
        "# Display table\n",
        "print(\"\\nğŸ¯ TOP 20 REDISTRIBUTION HOTSPOTS:\")\n",
        "print(\"=\" * 80)\n",
        "top_hotspots[['district', 'state', 'hotspot_score', 'msi_mean', 'msi_max', 'event_count', 'avg_spatial_spread']].round(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 4: TEMPORAL TREND ANALYSIS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating Temporal Trend Analysis...\")\n",
        "\n",
        "# Aggregate MSI over time\n",
        "temporal_msi = msi_df.groupby('time_period').agg({\n",
        "    'msi_score': ['mean', 'std', 'max', 'count'],\n",
        "    'activity_level': 'sum',\n",
        "    'geo_key': 'nunique'\n",
        "}).reset_index()\n",
        "temporal_msi.columns = ['time_period', 'msi_mean', 'msi_std', 'msi_max', 'event_count', 'total_activity', 'active_locations']\n",
        "\n",
        "fig_temporal = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    subplot_titles=(\n",
        "        \"ğŸ“ˆ MSI Trend Over Time (with confidence band)\",\n",
        "        \"ğŸ“Š Activity Volume & Active Locations\"\n",
        "    ),\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.12\n",
        ")\n",
        "\n",
        "# Confidence band (Â±1 std)\n",
        "fig_temporal.add_trace(\n",
        "    go.Scatter(\n",
        "        x=temporal_msi['time_period'],\n",
        "        y=temporal_msi['msi_mean'] + temporal_msi['msi_std'],\n",
        "        mode='lines',\n",
        "        line=dict(width=0),\n",
        "        showlegend=False,\n",
        "        hoverinfo='skip'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "fig_temporal.add_trace(\n",
        "    go.Scatter(\n",
        "        x=temporal_msi['time_period'],\n",
        "        y=temporal_msi['msi_mean'] - temporal_msi['msi_std'],\n",
        "        mode='lines',\n",
        "        line=dict(width=0),\n",
        "        fill='tonexty',\n",
        "        fillcolor='rgba(255, 153, 51, 0.2)',\n",
        "        showlegend=False,\n",
        "        hoverinfo='skip'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Mean MSI line\n",
        "fig_temporal.add_trace(\n",
        "    go.Scatter(\n",
        "        x=temporal_msi['time_period'],\n",
        "        y=temporal_msi['msi_mean'],\n",
        "        mode='lines+markers',\n",
        "        name='Mean MSI',\n",
        "        line=dict(color=INDIA_COLORS['saffron'], width=3),\n",
        "        marker=dict(size=8, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Max MSI line\n",
        "fig_temporal.add_trace(\n",
        "    go.Scatter(\n",
        "        x=temporal_msi['time_period'],\n",
        "        y=temporal_msi['msi_max'],\n",
        "        mode='lines+markers',\n",
        "        name='Max MSI',\n",
        "        line=dict(color=INDIA_COLORS['green'], width=2, dash='dash'),\n",
        "        marker=dict(size=6, color=INDIA_COLORS['green'])\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Activity bars\n",
        "fig_temporal.add_trace(\n",
        "    go.Bar(\n",
        "        x=temporal_msi['time_period'],\n",
        "        y=temporal_msi['total_activity'],\n",
        "        name='Total Activity',\n",
        "        marker=dict(color=INDIA_COLORS['green'], opacity=0.7)\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig_temporal.update_layout(\n",
        "    template=INDIA_TEMPLATE,\n",
        "    title=dict(\n",
        "        text=\"ğŸ“… TEMPORAL MSI ANALYSIS: When Did Redistribution Patterns Occur?\",\n",
        "        font=dict(size=20, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    height=700,\n",
        "    legend=dict(\n",
        "        orientation='h',\n",
        "        yanchor='bottom',\n",
        "        y=1.02,\n",
        "        xanchor='center',\n",
        "        x=0.5\n",
        "    ),\n",
        "    xaxis2=dict(tickangle=45)\n",
        ")\n",
        "\n",
        "fig_temporal.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 5: STATE-WISE MSI COMPARISON\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“Š Creating State Comparison...\")\n",
        "\n",
        "# State-level statistics\n",
        "state_stats = msi_df.groupby('state').agg({\n",
        "    'msi_score': ['mean', 'max', 'std'],\n",
        "    'activity_level': 'mean',\n",
        "    'spatial_spread': 'mean',\n",
        "    'district': 'nunique'\n",
        "}).reset_index()\n",
        "state_stats.columns = ['state', 'msi_mean', 'msi_max', 'msi_std', 'avg_activity', 'avg_spread', 'num_districts']\n",
        "state_stats = state_stats.sort_values('msi_mean', ascending=True)\n",
        "\n",
        "fig_states = go.Figure()\n",
        "\n",
        "# Mean MSI bars\n",
        "fig_states.add_trace(go.Bar(\n",
        "    y=state_stats['state'],\n",
        "    x=state_stats['msi_mean'],\n",
        "    orientation='h',\n",
        "    name='Mean MSI',\n",
        "    marker=dict(\n",
        "        color=state_stats['msi_mean'],\n",
        "        colorscale=INDIA_COLORSCALE,\n",
        "        line=dict(width=0)\n",
        "    ),\n",
        "    text=[f\"{v:.4f}\" for v in state_stats['msi_mean']],\n",
        "    textposition='outside',\n",
        "    textfont=dict(color=INDIA_COLORS['text'], size=9)\n",
        "))\n",
        "\n",
        "# Max MSI markers\n",
        "fig_states.add_trace(go.Scatter(\n",
        "    y=state_stats['state'],\n",
        "    x=state_stats['msi_max'],\n",
        "    mode='markers',\n",
        "    name='Max MSI',\n",
        "    marker=dict(\n",
        "        size=12,\n",
        "        color=INDIA_COLORS['chakra_blue'],\n",
        "        symbol='diamond',\n",
        "        line=dict(color=INDIA_COLORS['white'], width=1)\n",
        "    )\n",
        "))\n",
        "\n",
        "fig_states.update_layout(\n",
        "    template=INDIA_TEMPLATE,\n",
        "    title=dict(\n",
        "        text=\"ğŸ—ºï¸ STATE-WISE MSI COMPARISON: Which States Show Strongest Signals?\",\n",
        "        font=dict(size=20, color=INDIA_COLORS['saffron'])\n",
        "    ),\n",
        "    xaxis_title=\"MSI Score\",\n",
        "    yaxis_title=\"\",\n",
        "    height=1000,\n",
        "    legend=dict(\n",
        "        orientation='h',\n",
        "        yanchor='bottom',\n",
        "        y=1.02,\n",
        "        xanchor='center',\n",
        "        x=0.5\n",
        "    ),\n",
        "    barmode='overlay'\n",
        ")\n",
        "\n",
        "fig_states.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒŠ WAVE PATTERN DETECTION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸŒŠ Detecting wave propagation patterns...\")\n",
        "\n",
        "MIN_DURATION = 3  # Minimum weeks for a wave\n",
        "MIN_SPREAD = 3    # Minimum districts affected\n",
        "\n",
        "waves = []\n",
        "high_msi = msi_df[msi_df['msi_score'] > 0.3].copy()\n",
        "\n",
        "for state in high_msi['state'].unique():\n",
        "    state_data = high_msi[high_msi['state'] == state].copy()\n",
        "    state_data = state_data.sort_values('time_period')\n",
        "    \n",
        "    time_periods_list = sorted(state_data['time_period'].unique())\n",
        "    \n",
        "    for start_idx, start_time in enumerate(time_periods_list[:-MIN_DURATION]):\n",
        "        # Get initial districts\n",
        "        initial = set(state_data[state_data['time_period'] == start_time]['district'].unique())\n",
        "        \n",
        "        if len(initial) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Track spread\n",
        "        affected = {start_time: initial}\n",
        "        cumulative = initial.copy()\n",
        "        \n",
        "        for t in time_periods_list[start_idx + 1:start_idx + MIN_DURATION + 3]:\n",
        "            if t in state_data['time_period'].values:\n",
        "                new = set(state_data[state_data['time_period'] == t]['district'].unique())\n",
        "                affected[t] = new\n",
        "                cumulative = cumulative | new\n",
        "        \n",
        "        # Check if wave pattern\n",
        "        counts = [len(d) for d in affected.values()]\n",
        "        \n",
        "        if len(cumulative) >= MIN_SPREAD and len(counts) > 1 and max(counts) > counts[0]:\n",
        "            # Found a potential wave\n",
        "            peak_idx = np.argmax(counts)\n",
        "            peak_time = list(affected.keys())[peak_idx]\n",
        "            \n",
        "            waves.append({\n",
        "                'state': state,\n",
        "                'start_time': start_time,\n",
        "                'peak_time': peak_time,\n",
        "                'duration': len(affected),\n",
        "                'origin_districts': list(initial),\n",
        "                'total_affected': len(cumulative),\n",
        "                'all_districts': list(cumulative),\n",
        "                'spread_sequence': {str(k): list(v) for k, v in affected.items()},\n",
        "                'peak_count': max(counts),\n",
        "                'wave_score': len(cumulative) * max(counts) / (len(initial) + 1)\n",
        "            })\n",
        "\n",
        "waves = sorted(waves, key=lambda x: x['wave_score'], reverse=True)\n",
        "\n",
        "print(f\"\\nâœ… Detected {len(waves)} wave patterns\")\n",
        "\n",
        "if waves:\n",
        "    print(\"\\nğŸŒŠ TOP 10 WAVE PATTERNS:\")\n",
        "    print(\"=\" * 80)\n",
        "    for i, wave in enumerate(waves[:10], 1):\n",
        "        print(f\"\\n{i}. {wave['state']}\")\n",
        "        print(f\"   Timeline: {wave['start_time']} â†’ {wave['peak_time']}\")\n",
        "        print(f\"   Origin: {', '.join(wave['origin_districts'][:3])}{'...' if len(wave['origin_districts']) > 3 else ''}\")\n",
        "        print(f\"   Affected: {wave['total_affected']} districts | Wave Score: {wave['wave_score']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š VISUALIZATION 6: WAVE PATTERN ANALYSIS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "if waves:\n",
        "    print(\"ğŸ“Š Creating Wave Pattern Visualization...\")\n",
        "    \n",
        "    # Visualize top wave\n",
        "    top_wave = waves[0]\n",
        "    \n",
        "    fig_wave = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=(\n",
        "            f\"ğŸŒŠ Wave Score Gauge\",\n",
        "            \"ğŸ“ˆ District Spread Over Time\",\n",
        "            \"ğŸ—ºï¸ Affected Districts Timeline\",\n",
        "            \"ğŸ“Š Cumulative Wave Intensity\"\n",
        "        ),\n",
        "        specs=[\n",
        "            [{\"type\": \"indicator\"}, {\"type\": \"bar\"}],\n",
        "            [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Wave score gauge\n",
        "    fig_wave.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"gauge+number\",\n",
        "            value=top_wave['wave_score'],\n",
        "            title={'text': \"Wave Score\", 'font': {'color': INDIA_COLORS['text']}},\n",
        "            gauge={\n",
        "                'axis': {'range': [0, max(top_wave['wave_score'] * 1.5, 10)]},\n",
        "                'bar': {'color': INDIA_COLORS['saffron']},\n",
        "                'bgcolor': INDIA_COLORS['background'],\n",
        "                'borderwidth': 2,\n",
        "                'bordercolor': INDIA_COLORS['saffron'],\n",
        "                'steps': [\n",
        "                    {'range': [0, top_wave['wave_score']/3], 'color': INDIA_COLORS['green']},\n",
        "                    {'range': [top_wave['wave_score']/3, top_wave['wave_score']*2/3], 'color': INDIA_COLORS['white']},\n",
        "                    {'range': [top_wave['wave_score']*2/3, top_wave['wave_score']*1.5], 'color': INDIA_COLORS['saffron']}\n",
        "                ]\n",
        "            },\n",
        "            number={'font': {'color': INDIA_COLORS['saffron']}}\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # District counts over time\n",
        "    spread_data = top_wave['spread_sequence']\n",
        "    times = list(spread_data.keys())\n",
        "    counts = [len(spread_data[t]) for t in times]\n",
        "    \n",
        "    fig_wave.add_trace(\n",
        "        go.Bar(\n",
        "            x=times,\n",
        "            y=counts,\n",
        "            marker=dict(\n",
        "                color=counts,\n",
        "                colorscale=INDIA_COLORSCALE\n",
        "            ),\n",
        "            text=counts,\n",
        "            textposition='outside',\n",
        "            textfont=dict(color=INDIA_COLORS['text'])\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # Timeline scatter\n",
        "    for i, (t, dists) in enumerate(spread_data.items()):\n",
        "        fig_wave.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[t] * len(dists),\n",
        "                y=list(range(len(dists))),\n",
        "                mode='markers+text',\n",
        "                marker=dict(\n",
        "                    size=12,\n",
        "                    color=INDIA_COLORS['saffron'] if i == 0 else INDIA_COLORS['green'],\n",
        "                    symbol='circle'\n",
        "                ),\n",
        "                text=dists,\n",
        "                textposition='middle right',\n",
        "                textfont=dict(size=9, color=INDIA_COLORS['text']),\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "    \n",
        "    # Cumulative intensity\n",
        "    cumulative_counts = np.cumsum(counts)\n",
        "    fig_wave.add_trace(\n",
        "        go.Bar(\n",
        "            x=times,\n",
        "            y=cumulative_counts,\n",
        "            marker=dict(\n",
        "                color=cumulative_counts,\n",
        "                colorscale=[[0, INDIA_COLORS['green']], [1, INDIA_COLORS['saffron']]]\n",
        "            ),\n",
        "            text=cumulative_counts,\n",
        "            textposition='outside',\n",
        "            textfont=dict(color=INDIA_COLORS['text'])\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    fig_wave.update_layout(\n",
        "        template=INDIA_TEMPLATE,\n",
        "        title=dict(\n",
        "            text=f\"ğŸŒŠ WAVE PATTERN ANALYSIS: {top_wave['state']} ({top_wave['start_time']} â†’ {top_wave['peak_time']})\",\n",
        "            font=dict(size=20, color=INDIA_COLORS['saffron'])\n",
        "        ),\n",
        "        height=700,\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    fig_wave.show()\n",
        "else:\n",
        "    print(\"âš ï¸ No significant wave patterns detected with current thresholds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4ï¸âƒ£ Key Findings & Insights\n",
        "\n",
        "### Interpretation Guidelines\n",
        "\n",
        "| MSI Score | Interpretation |\n",
        "|-----------|----------------|\n",
        "| > 0.5 | **Very Strong** redistribution signal |\n",
        "| 0.3 - 0.5 | **Strong** redistribution signal |\n",
        "| 0.1 - 0.3 | **Moderate** signal, may be noise |\n",
        "| < 0.1 | **Weak/No** significant pattern |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“‹ FINAL SUMMARY & KEY INSIGHTS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ‡®ğŸ‡³ MOBILITY SIGNAL INDEX ANALYSIS â€” FINAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nğŸ“Š DATASET OVERVIEW:\")\n",
        "print(f\"   â€¢ Total records analyzed: {len(enrolment_df) + len(demographic_df) + len(biometric_df):,}\")\n",
        "print(f\"   â€¢ Time period: {enrolment_df['date'].min().strftime('%b %Y')} - {enrolment_df['date'].max().strftime('%b %Y')}\")\n",
        "print(f\"   â€¢ States covered: {len(states)}\")\n",
        "print(f\"   â€¢ Districts analyzed: {msi_df['geo_key'].nunique()}\")\n",
        "\n",
        "print(\"\\nğŸ§® MSI STATISTICS:\")\n",
        "print(f\"   â€¢ Total MSI measurements: {len(msi_df):,}\")\n",
        "print(f\"   â€¢ Average MSI: {msi_df['msi_score'].mean():.4f}\")\n",
        "print(f\"   â€¢ Maximum MSI: {msi_df['msi_score'].max():.4f}\")\n",
        "print(f\"   â€¢ High-MSI events (>0.3): {len(msi_df[msi_df['msi_score'] > 0.3]):,}\")\n",
        "\n",
        "print(\"\\nğŸ¯ TOP 5 REDISTRIBUTION HOTSPOTS:\")\n",
        "for i, (_, row) in enumerate(top_hotspots.head(5).iterrows(), 1):\n",
        "    print(f\"   {i}. {row['district']}, {row['state']}\")\n",
        "    print(f\"      Score: {row['hotspot_score']:.4f} | Mean MSI: {row['msi_mean']:.4f} | Max MSI: {row['msi_max']:.4f}\")\n",
        "\n",
        "if waves:\n",
        "    print(f\"\\nğŸŒŠ WAVE PATTERNS DETECTED: {len(waves)}\")\n",
        "    print(\"   Top wave:\")\n",
        "    print(f\"   â€¢ State: {waves[0]['state']}\")\n",
        "    print(f\"   â€¢ Timeline: {waves[0]['start_time']} â†’ {waves[0]['peak_time']}\")\n",
        "    print(f\"   â€¢ Districts affected: {waves[0]['total_affected']}\")\n",
        "    print(f\"   â€¢ Wave score: {waves[0]['wave_score']:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âš ï¸  IMPORTANT DISCLAIMER:\")\n",
        "print(\"   These findings indicate STATISTICAL PATTERNS consistent with redistribution.\")\n",
        "print(\"   They do NOT prove actual population movement.\")\n",
        "print(\"   Possible alternative explanations include:\")\n",
        "print(\"   â€¢ Enrollment camp effects\")\n",
        "print(\"   â€¢ Seasonal variations\")\n",
        "print(\"   â€¢ Administrative changes\")\n",
        "print(\"   â€¢ Data collection variations\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ’¾ EXPORT RESULTS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ’¾ Exporting results...\")\n",
        "\n",
        "# Export MSI results\n",
        "msi_df.to_csv('msi_results.csv', index=False)\n",
        "print(\"   âœ“ msi_results.csv\")\n",
        "\n",
        "# Export hotspots\n",
        "hotspot_stats.to_csv('redistribution_hotspots.csv', index=False)\n",
        "print(\"   âœ“ redistribution_hotspots.csv\")\n",
        "\n",
        "# Export wave patterns\n",
        "if waves:\n",
        "    waves_df = pd.DataFrame(waves)\n",
        "    waves_df.to_csv('wave_patterns.csv', index=False)\n",
        "    print(\"   âœ“ wave_patterns.csv\")\n",
        "\n",
        "# Save visualizations as HTML\n",
        "output_dir = Path('msi_visualizations')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "fig_summary.write_html(output_dir / 'summary_dashboard.html')\n",
        "fig_heatmap.write_html(output_dir / 'msi_heatmap.html')\n",
        "fig_hotspots.write_html(output_dir / 'hotspot_ranking.html')\n",
        "fig_temporal.write_html(output_dir / 'temporal_analysis.html')\n",
        "fig_states.write_html(output_dir / 'state_comparison.html')\n",
        "if waves:\n",
        "    fig_wave.write_html(output_dir / 'wave_analysis.html')\n",
        "\n",
        "print(f\"   âœ“ All visualizations saved to {output_dir}/\")\n",
        "\n",
        "print(\"\\nâœ… Export complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š Technical Appendix\n",
        "\n",
        "### MSI Algorithm Summary\n",
        "\n",
        "```\n",
        "For each location A and time period t:\n",
        "1. Get activity changes for A over window W\n",
        "2. Get average activity changes for A's neighbors over window W\n",
        "3. Compute inverse correlation: Ï_inv = -corr(Î”A, Î”neighbors)\n",
        "4. Compute spatial spread: S = (# neighbors moving opposite) / (# neighbors)\n",
        "5. Compute anomaly magnitude: Z = (activity - rolling_mean) / rolling_std\n",
        "6. Final MSI = Ï_inv Ã— (1 + S) Ã— min(|Z|, 3) / 3\n",
        "```\n",
        "\n",
        "### Wave Detection Algorithm\n",
        "\n",
        "```\n",
        "For each state:\n",
        "1. Find time periods with high-MSI events\n",
        "2. For each potential start time:\n",
        "   a. Track which districts show high MSI\n",
        "   b. Track spread to new districts in subsequent periods\n",
        "   c. If total affected â‰¥ threshold AND pattern shows growth â†’ flag as wave\n",
        "3. Score waves by: total_affected Ã— peak_count / initial_count\n",
        "```\n",
        "\n",
        "### Geographic Neighbor Definition\n",
        "\n",
        "In absence of explicit coordinates, neighbors are defined as:\n",
        "- All districts within the same state\n",
        "- (Optional enhancement: Use pincode prefix similarity for finer granularity)\n",
        "\n",
        "---\n",
        "\n",
        "**Created for UIDAI Aadhaar Hackathon 2025**\n",
        "\n",
        "ğŸ‡®ğŸ‡³ *Theme: Indian Tricolor â€” Saffron â€¢ White â€¢ Green*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
